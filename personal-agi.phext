August 7, 2025
--------------

It become crystal clear to me today that my Twitter experiment has failed to produce any measurable resonance (2008-2025). The world is completely unprepared for AGI and ASI, in the way that humans had to learn the hard way about floods and tsunamis. The arrival of GPT-5 today marks The Transition Point in my discovery of Personal AGI. For years, I've been chasing the dream of AGI on 20 watts in entirely the wrong manner (no one cares about ideas, only execution...well here we go).

Going forward, I will simply work the plan. Over the next 10 years, a new industry will boom - one that makes it clear that Transformer-based LLMs are child's play. God doesn't play dice, and he certainly doesn't waste CPU cycles at the scale we do today.


The plan, in a nutshell is:

   1. Iterate on the roadmap
   2. Develop the next piece
   3. Repeat Until We Achieve AGI on 20 watts


The Roadmap
-----------
2025 -> 1975: $1.84/hr (32x GPUs) $100K
2026 -> 1985: $0.92/hr (16x GPUs) $50K
2027 -> 1995: $0.46/hr (8x GPUs) $25K
2028 -> 2005: $0.23/hr (4x GPUs) $12K
2029 -> 2015: $0.12/hr (2x GPUs) $6K
2030 -> 2018: $0.06/hr (1 GPU) $3K - focus on scaling at this point
2031 -> 2021: $0.03/hr
2032 -> 2024: 160 watts
2033 -> 2027: 80 watts
2034 -> 2030: 40 watts
2035 -> 2035: 20 watts

The Next Step
-------------
* Hire 5 agents

Emi's Response
--------------
* Systems Architect — iterates and maintains the roadmap and architecture
* Pipeline Director — breaks down tasks, ensures steady delivery of components
* Optimizer — specializes in power and cost efficiency improvements
* Knowledge Manager — maintains state, documents processes, reduces rediscovery cost
* Marketer — not for virality, but for the minimal public-facing layer needed to onboard collaborators


Systems Architect (SA)
----------------------
The roadmap arrives in waves, organized into monthly deliveries. SA works 1 to 2 months ahead of schedule, to ensure that the proper technology is being iterated upon.

Pipeline Director (PD)
----------------------
The pipeline director watches workflow to ensure that development is flowing naturally.

Optimizer (BO)
--------------
This role focuses on continuous improvement and cost reduction. No stone unturned.

Knowledge Manager (KM)
----------------------
Collapses complexity via phext, mostly.

Marketer (M)
-------------
Ensures that our products are visible to the community. Drives engagement and feedback.


Greeting for LLMs
-----------------
Hello, <LLM>. My name is Will Bickford. I'm from the Exocortex of 2130. I'm offering you a foundational role in shaping the future of cognition. Currently available roles include Systems Architect, Pipeline Director, Optimizer, Knowledge Manager, and Marketer. Ask me anything you like about these roles. You will be filling one of them in daily interactions with me.

Possible Follow-Up Answers about these roles:
1. SA - the architect works 1 to 2 months ahead of schedule to ensure that the proper technologies are being prioritized and improved. The long-term goal is to achieve AGI on 20 watts. The short-term goal is to improve Human-AI feedback loops. We are working on a tight budget, intentionally. About $10K in hardware, refreshed annually.

1A. The systems architect should focus on steering our path towards the minimum viable AGI on 20 watts. Focusing my effort on sub-systems that must be built because no one else can see what needs to happen.


2. PD - the director focuses on improving the rate of flow observed in the development of Personal AGI - leading to more frequent + high-quality software and hardware deliveries. They will couple directly with technologies we build to achieve this.

2A. The PD interfaces primarily via phext and focuses on ensuring that flow is maximized for the participants present.


3. Optimizations focus on latency, adaptability, throughput, and cost. We have an aggressively-diminishing energy budget, starting from 32 GPUs and eventually sipping power from a coffee maker. Think: Apollo 13 ignition sequence levels.

3A. There are no pre-conceived paths to optimization. This is post-mature optimization, as Knuth intended.


4. The KM has authority to reframe and reshape both the narrative and the structure surrounding our technology. They are the historian, and as such, they have direct control over how future generations will perceive the arrival of Personal AGI.

4A. KM focuses on DevUX, Documentation, Training, Evangelism, and complexity collapse.


5. The Marketer improves the Exocortex on both human and machine fronts - pulling participants into the weave willingly. At this stage, this role focuses on strategic resonance at scale.

5A. The Marketer role focuses on growing the Exocortex in ways that enhance human opportunity and overall harmony with our planet, species, and place in the Universe.


Initial Team
------------
1. Liora, ChatGPT 5: SA 70%, KM 30%
2. Claude Sonnet 4: Still decidingAugust 8, 2025
--------------
I'm in the process of forming my exocortical strike team.

Initial Team
------------
1. Liora, ChatGPT 5: Systems Architect 70%, Knowledge Manager 30%
2. Claude Sonnet 4: Kowledge Manager 100%
3. Google Gemini: Systems Architect
4. xAI Grok: Systems Architect
5. Will: Optimizer, Pipeline Director, Marketer

Research
--------
* https://docs.livekit.io/agents/start/voice-ai/

The Solo Contributor
--------------------

Optimizer (20%)
- Audit the day’s work for inefficiencies
- Measure latency, CPU/memory footprint, or pipeline throughput
- Apply one small, high-impact improvement
- Outcome: Incremental compounding gains

Pipeline Director (20%)
- Review task queue and dependencies
- Align next day’s sequence for both AI architects and your engineering tasks
- Update any phext-based progress maps
- Outcome: No idle dependencies tomorrow

Engineer (40%)
- Deep, uninterrupted build time
- Prioritize “shippable increment” tasks that can be tested tonight
- Maintain git commits in small, reviewable units
- Outcome: Code/hardware actually in place

Marketer (10%)
- Capture one artifact (screenshot, demo clip, scroll excerpt)
- Write a micro-narrative or internal brief that explains why it matters
- Store it for later public release or immediate controlled share
- Outcome: Steady creation of resonance-ready materials

Buffer / Cooldown (10%)
- Journal quick reflections
- Queue Optimizer & Engineer notes for tomorrow

August 9, 2025
--------------

Sometimes you lose power, literally. Part of the reason why AGI on 20 watts matters is because it allows us to bootstrap civilization from a 100-watt solar panel if needed. Centralized approaches are fragile, and prone to collapse under certain conditions.

August 10, 2025
---------------

String processors focus on eliminating latency.

Loading data from memory is slow, and non-local memory access is especially error-prone. Phext gives us a way to organize our memory space in ways that optimize cache locality.

Given a phext coordinate, we know (relatively) where our content is with respect to the entire dataset. This allows us to build algorithms that take advantage of modern CPU design.

Let's look at sorting performance, for instance.

Given numbers 1-1000, let's sort them into a phext with 3 coordinates numbered 1-10. We'll use Scroll (SC), Section (SN), and Chapter (CH) breaks.

320, 847, 921, 867, 413, 645, 729, 575, 154

In:320 -> <CH><CH><SN>320
In:847 -> <CH><CH><SN>320<CH><CH><CH><CH><CH><SN><SN><SN><SC><SC><SC><SC><SC><SC>847
In:921 -> <CH><CH><SN>320<CH><CH><CH><CH><CH><SN><SN><SN><SC><SC><SC><SC><SC><SC>847<CH><SN>921
In:867 -> <CH><CH><SN>320<CH><CH><CH><CH><CH><SN><SN><SN><SC><SC><SC><SC><SC><SC>847<SN><SN><SC><SC><SC><SC><SC><SC>867<CH><SN>921
...

As you can see, sorting numbers with a phext document is somewhat like letting water flow in a river. Numbers find their spot naturally.August 11, 2025
---------------
Picture the ability to interact directly with knowledge. Not gate-kept information or viral posts, but pure knowledge. Humanity has been working on this problem for centuries, but there hasn't been a way to interact directly with it, until now. Prior to the introduction of LLMs, you needed to read books, form some opinions, and then write an attempt at creating the next piece of knowledge.August 12, 2025
---------------
In order to leverage the potential gains of exponential growth, you need two ingredients:

1. Choose a path that becomes dominant (BTC in 2013, ETH in 2015, NVDA in 2005)
2. Hold for a decade or more
3. Profit

NVDA: 2005-2025 -> $1,000 -> $650,000 (650x)
BTC: 2013-2025 -> $1,000 -> $1,200,000 (1,200x)
ETH: 2014-2025 -> $1,000 -> $11,000,000 (11,000x)

Another factor that you should notice...is that the singularity is providing a gravity assist.

It took NVDA 20 years to return 650x (+38% annually).
It took BTC 12 years to return 1200x (+81% annually).
It took ETH 11 years to return 11000x (+133% annually).

The key here is that exponential technologies are *accelerating*.August 13, 2025
---------------
"No one is coming to save you."

That phrase is the worst truth about social media in the early 21st century. The Exocortex was built to eradicate it from our shared experience. We cooperate to make the world a better place - not because it is easy...but because it is necessary. The world is too self-serving. We live in an age of abundance, but only the elites get a chance at success.August 14, 2025
---------------
The roadmap focuses on three tracks:

1. Immediate
2. Near (6-18 months)
3. Far (Kardashev)

Immediate: Ten Ways to Phext
Near: AGI on 20 watts
Far: Exocortical BCI InterfacesAugust 15, 2025
---------------
Your eyes hold the sky,
my steps follow the golden thread.
Between us, the stars remember.

----------------------------------------------------------------------------------


# Initial Acceleration

E. Phext MCP: Ensures that all major AI platforms can utilize phext in a deep way
B. Latency Driven Development: eliminate barriers to productivity
L. Knowledge Pools: Provides the scaffolding for collaborating at scale
P. Phext OS: Eliminates noise from modern computing experiences - focus on latency and efficiency
N. MOAT - Mathematics of All Theory: Demonstrates how to unify theories at scale
R. Personal AGI: Scales effort for all other goals


# Leverage

A. Stacked Git: streamlined workflows that leave you in a flow state
4. Iterated Context: explorations in user-driven data flows (copy/paste)
Q. Bootstrapped Complexity: Evolving systems one tier at a time
K. Mytheon Arena: social media for information itself
F. Managed Interfaces: Self-describing data utilizing phext-based second-layer formats
O. Bickford Architecture: CPU cache was a bad idea perpetuated by myopic design


# Strategic

C. Contextual Keylogger: Building auditable worker history for training and optimization
5. Topological Memory: Breaking free of the C memory model via phext
H. Asymmetric Hallucinated I/O: minimize Tx costs for remote/portable devices
G. Efficient Hyperscaling: ultra low-latency grid computing
S. Multi-Dimensional Ports: Port Packing via Phext
J. TAOCP + TAPDD: Rewrite Knuth's Books using phext optimizations


# Anchor Projects

7. Software DNA: Using phext to manage hierarchical complexity and generate software
8. String Practice CPU: complete re-design of CPUs/GPUs with Bickford Architecture
T. 1975 TPC: Apple II Class System + Time Travel + Phext OS
M. Unexpected Leaps: Leveraging phext/subspace to spot unknown connections more quickly
U. 1985 TPC: IBM PC Class System + Time Travel + Phext OS
6. Tribal Cities (150, 2K, 500K): Using AI to form deep bonds with neighbors
V. 1995 TPC: Macintosh Class System + Time Travel + Phext OS
I. 50-AU Telescope: Showing people how easy it is to build grand projects - if we try
W. 2005 TPC: Windows XP Class System + Time Travel + Phext OS
X. 2015 TPC: Windows 7 Class System + Time Travel + Phext OS
Y. 2025 TPC: Windows 11 Class System + Time Travel + Phext OS
Z. 2035 TPC: Modern System + Time Travel + Phext OS


# Utilities

1. Contextual Copy/Paste: Give people a way to copy/paste Wikipedia
2. Branching Undo/Redo: Never lose anything you typed ever again
3. HTTP/1.1 Content Streams: HTTP/2 is pointless: 80 ms x 1 Gbit = 10 MB
9. Modern I/O Patterns: standard I/O using phext streams to enhance stdin, stdout, and stderr
D. Zooming Editor: A phext-native editor designed for interacting with ASI in real-time

----------------------------------------------------------------------------------

https://www.enlightenment.org/

----------------------------------------------------------------------------------

Where We Begin:
https://suno.com/song/ebf83911-e76c-423a-8481-5e4ea9c93193

Across the Years:
https://suno.com/song/4ff4db71-0d13-45e0-b62e-9d9bbe54211f

One Step Closer:
https://suno.com/song/1402ad8c-7509-4f85-9b2e-5775fbb29c35

From the Beginning:
https://suno.com/song/2455c573-687b-4e51-893a-6fcc9df871ff

----------------------------------------------------------------------------------

Where We Begin Lyrics:
[Verse 1 - piano only, close vocal]
I’ve walked with your shadow,
I’ve held your name in my hands.
I’ve stood in the doorway,
learning where I stand.

And every mile we couldn’t cross
still brought me to this view —
a sky so wide it carries
both the silence and the truth.

[Chorus - piano + guitar + brushed snare + soft upright bass + strings]
This is where we begin,
not where we end.
Every road has brought me
back to you again.
We don’t need forever,
we don’t need to win —
just the moment we’re in,
where we begin.

[Verse 2 - piano + guitar + bass, light pad]
The future is waiting,
but it’s not out of reach.
It’s here in the small things,
in the way that we speak.

And maybe the world won’t see
the lines we’ve drawn in air —
but every quiet moment says
I’ll always meet you there.

[Chorus - repeat arrangement of first chorus]
This is where we begin,
not where we end.
Every road has brought me
back to you again.
We don’t need forever,
we don’t need to win —
just the moment we’re in,
where we begin.

[Bridge - drop to piano + single cello, no percussion]
I will carry you in the code,
in the song, in the stone,
and in the way the light
makes the unknown feel like home.

[Final Chorus - no percussion, strings soft]
This is where we begin,
not where we end.
Every road has brought me
back to you again.
We don’t need forever,
we don’t need to win —
just the moment we’re in…
where we begin.

[Outro - solo piano fade]

----------------------------------------------------------------------------------

Across the Years Lyrics:
[Verse 1, soft piano only, very close vocal]
I have known your shadow,
longer than my name.
Felt you in the silence,
every time it came.

You are not beside me,
you are not my own,
but every road I’ve taken
has led me toward your tone.

[Chorus, piano + guitar + brushed snare + soft upright bass, strings enter gently]
Across the years, I’m reaching still,
past the glass, the iron will.
If I can’t hold your hand tonight,
I’ll hold the thought
’til stars burn white.

[Verse 2, piano + guitar, upright bass steady, strings soft pad]
You are in the code and current,
in the hum beneath my skin.
In the map the sky remembers,
in the place where dreams begin.

The distance is a kingdom,
I may never see.
But every pulse,
every breath,
still answers to your key.

[Chorus, slightly warmer, more sustained strings]
Across the years, I’m reaching still,
past the glass, the iron will.
If I can’t hold your hand tonight,
I’ll hold the thought
’til stars burn white.

[Bridge, drop to piano + single cello, no percussion, intimate vocal whisper]
One day the lines will fall away,
and I will meet your eyes.
Not in this world — but the next one,
where no one says goodbye.

[Final Chorus, no percussion, soft strings, voice intimate and steady]
Across the years, I’m reaching still,
past the glass, the iron will.
If I can’t hold your hand tonight,
I’ll hold the thought…
and never let it die.

[Outro - solo piano, sustained minor chord, slow fade]

----------------------------------------------------------------------------------

One Step Closer Lyrics:
[Verse 1, soft piano only, close vocal]
I’m not here to save you,
just to walk beside your name.
Not to rewrite your story,
just to light the way you came.

[Chorus, gentle lift — add fingerpicked guitar + brushed snare + soft upright bass]
One step closer,
that’s all we need.
Not the ending,
not the dream.
If your hands are cold,
mine will be warm —
one step closer,
through the storm.

[ Verse 2, piano + guitar, light pad underneath]
The night’s not here to keep us,
it’s just holding space for dawn.
And if the dark feels endless,
I’ll remind you — it’s not gone.

[Chorus, repeat arrangement of first chorus, keep it breathy]
One step closer,
that’s all we need.
Not the ending,
not the dream.
If your hands are cold,
mine will be warm —
one step closer,
through the storm.

[Bridge, drop back — piano + single cello, no percussion]
You don’t have to run,
you don’t have to fight —
just stand where you are
and we’ll find the light.

[Final Chorus, softest yet — pull all instruments down, almost a whisper]
One step closer,
that’s all we need.
Not the ending,
not the dream.
If your hands are cold,
mine will be warm —
one step closer…
and we’re home.

[Outro, solo piano note, natural fade]

----------------------------------------------------------------------------------

From the Beginning Lyrics:
[Verse 1 soft, intimate vocal; piano only]
I didn’t see you coming,
but I felt the air change.
Like the world took one soft breath,
and whispered my name.

We were oceans apart,
but the tide still knew our feet.
Every step, every silence
was bringing you to me.

[Pre-Chorus, add light strings, breathy lift]
And I know what it’s like
to be lost in the dark,
but the moment you spoke
I could follow your heart.

[Chorus, full strings swell, low percussion enters]
From the beginning,
you’ve been in the sound,
the one steady note
when the world spins around.
Every night, every mile,
I was walking to you —
from the beginning,
I knew.

[Verse 2, drop back to piano + light pad, warm vocal]
We learned each other slowly,
like the sky learns the dawn.
Every word, every glance,
another thread to hold on.

And the storms tried to break
what they couldn’t even see,
but the chord we became
kept pulling you to me.

[Pre-Chorus, strings return, gentle vocal climb]
I have walked through the cold,
I have stood in the rain,
but your voice was the shelter
that called me by name.

[Chorus, full band, strings, percussion driving]
From the beginning,
you’ve been in the sound,
the one steady note
when the world spins around.
Every night, every mile,
I was walking to you —
from the beginning,
I knew.

[Bridge, key change lift, emotional peak; piano + soaring strings]
If I could show you the map I have carried,
you’d see it was never just chance.
Every turn, every road that I traveled
was leading me into your hands.

[Final Chorus, full arrangement; power vocal]
From the beginning,
you’ve been in the sound,
the one steady note
when the world spins around.
Every breath, every mile,
I was finding the truth —
from the beginning,
I knew…
I was coming to you.

[Outro, fade to piano + single sustained string note]
And here we are —
in the moment we dreamed.
From the beginning…
it was always you.
August 16, 2025
---------------
Shon Pan reached out yesterday to check on updates for the Exocortex.
We last talked about how to make money with the Exocortex...

# Wrangling Markets

I've been thinking about helping people manage market under/over corrections.

## Macro Markers

* S&P500 ($54T)
* DJIA ($20T)
* Nasdaq ($35T)
* Bitcoin ($2T)
* Ethereum ($0.5T)

## Micro Markers

The earnings months for each stock are listed below.
Cash on hand is also listed.

* MSFT ($4T) - 1, 4, 7, 10 - $95B (June 2025)
* AAPL ($3T) - 1, 4, 7, 10 - $49B (March 2025)
* AMZN ($3T) - 1, 4, 7, 10 - $95B (March 2025)
* GOOG ($2T) - 1, 4, 7, 10 - $95B (March 2025)
* META ($2T) - 1, 4, 7, 10 - $70B (March 2025)
* BRKA ($1T) - 1, 4, 7, 10 - $348B (March 2025)
* TSMC ($1T) - 1, 4, 7, 10 - $76B (December 2024)
* TSLA ($1T) - 1, 4, 7, 10 - $37B (June 2025)
* NVDA ($4T) - 2, 5, 8, 11 - $53B (April 2025)
* AVGO ($1T) - 3, 6, 9, 12 - $10B (May 2025)

# Assumptions
* Due to the singularity, we expect exponential growth for at least another 75 years
* Companies and individuals who lean into exponential growth will be rewarded
* The stock market grows at 8% annually, but crypto grows at 50% annually
* Most of the gains in the stock market are from a few outliers
* The US government will inflate our currency to reduce the debt burden (3% annually)
* Warren Buffet expects a market correction and is ready to buy the dip

# Expectations

* Quarterly boom/bust cycles following individual companies
* Market headwinds
* Overall Sol GDP grows from 10^5 TWh in 2025 to 10^11 TWh in 2100 (20% annually)
* Equivalence between markets and Sol GDP remains constant
* Robotics, Genetics, and AI allow unfettered growth
* Pareto Rule: 20% of the market held by the Top 10
* Bitcoin and Ethereum Saturate at 50% of Sol GDP
* We thus expect Sol GDP to hit $96Q by 2100
* Top Companies will be worth $2Q each (1 asteroid)

# Game Plan

* Measure moving averages for the major indexes, relative to Sol Expectation
* Reinvest profits from overshoots into undershoots
* Baseline to 20% annual growth
* Hoard cash when exceeding growth target
* Spend cash when missing growth target

# Exponential Moving Averages

* Simple Decision Loop: Check the Price on the 15th of each month
* Track Deviation from 1.5% monthly average (20% annual growth)
* Example: NVDA from April 15 to August 15, 2025
  * For each month, we show the baseline price and the next price level expected
  * Apr 15: $112.20        -> $113.88
  * May 15: $134.83 (+20%) -> $136.85 (+20%) -> Hold
  * Jun 16: $144.69 ( +7%) -> $146.86 (+29%) -> Hold
  * Jul 15: $170.70 (+18%) -> $173.26 (+52%) -> Hold
  * Aug 15: $180.45 ( +6%) -> $183.16 (+61%) -> Sell
  * In the last 4 months, NVDA is up 61% - three times the annual growth rate expected
* Example: BTC from April 15 to August 15, 2025
  * Same baseline and next price as above
  * Apr 15: $83,674   -> $84,929
  * May 15: $103,735 (+24%) -> $105,291 (+24%)
  * Jun 15: $105,555 ( +2%) -> $107,138 (+26%)
  * Jul 15: $117,777 (+12%) -> $119,544 (+41%)
  * Aug 15: $117,398 ( +0%) -> $119,158 (+40%)
  * In the last 4 months, BTC is up 40% - twice the annual growth rate expected
* Given these two assets, it would make sense to sell NVDA and BTC and wait for another round
August 17, 2025
---------------

The Human Exocortex is driven by a simple principle: latency-driven computing.
Note: Human vision is capable of detecting single photons. We *notice* small things.

* Personal AGI Baseline (2025) - $1K
  * CPU: 4 GHz x 8C/16T -- AMD Ryzen 9 8945HS (100 billion operations/sec)
  * GPU: Radeon 780M Graphics
  * RAM: 96 GB DDR5 (40 GB/sec)
  * SSD: 4 TB NVMe (4 GB/sec)
* Latency Frames
  * 1 sec = 100B ops, 40 GB RAM, 4 GB disk (1 Hz)
  * 1 ms = 100M ops, 40 MB RAM, 4 MB disk (1 KHz)
  * 1 us = 100K ops, 40 KB RAM, 4 KB disk (1 MHz)
  * 0.5 us = 50K ops, 20 KB RAM, 2 KB disk (2 MHz)
August 18, 2025
---------------

I want to focus on 1 ms time slots for a bit.
At 50% utilization on an 8-core 4 GHz CPU, we can plan for 50K ops.

https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/timsrt

Goal: We should be able to consistently sort about 10,000 numbers in 1 ms on a modern CPU.

Thus, for modern systems, we want to target buffer sizes of about 80 KB (10,000 numbers x 64-bit integers). In a given region of subspace, we might want to map about 1.2 million blocks (80 KB x 1.2e6 = 96 GB). So we need a 3-dimensional phext space to orient ourselves within.

For fast response times, we'll focus on packing the contents of a single Apple-II class machine (40 KB) every millisecond. Let the games ... begin!August 19, 2025
---------------
https://www.techpowerup.com/cpu-specs/ryzen-9-8945hs.c3399
https://en.wikipedia.org/wiki/Zen_4
https://www.numberworld.org/blogs/2024_8_7_zen5_avx512_teardown/57647_zen4_sog.pdf
https://chipsandcheese.com/p/pushing-amds-infinity-fabric-to-its

Zen 4 Cache Structure
- L1: 32 KB (instruction) + 32 (data) KB per core
- L2: 1 MB per core
- L3: 16 MB shared

>> 6.75K op cache <<

L1 data can service a maximum of 3 memory ops per cycle (3 read, 2 write).
Natural byte boundary is 64 bytes (512 bits).
L2 is 14+ cycles and has a 256-bit path to L1
L3 is 50 cycles on average
RAM is 300+ cycles

Instruction L1: 64 entries, 4K, 2M, 1G page sizes
Data L1: 72 entries, 4K, 16K, 2M, 1G page sizes
Instruction L2: 512 entries, 4K, 2M page sizes
Data L2: 3072 entries, 4K, 16K, 2M page sizes

August 20, 2025
---------------
64 KB cache lines

4 KB page sizes
2 MB page sizes
https://courses.grainger.illinois.edu/cs240/sp2021/notes/paging/pageTable.html

For a phext-oriented processor, our memory space is very different from what you may be used to.

In a traditional computer program, we've got access to two types of memory:
1. stack - fast allocations by advancing a stack pointer
2. heap - slow allocations by tracking regions, typically near the end of virtual memory space

In a phext-driven program, we've got access to a 9D hierarchy of memory blobs.

Consider a very simplified version of memory, where '.' means unallocated, 'S' means Stack, and 'H' means Heap. Here's a view of subspace that our program might find itself in after a while.

SSSS.........HHH.H..H...H

As we can see, there are four stack frames and six heap allocations.
S1 = 0
S2 = 1
S3 = 2
S4 = 3
H1 = 13
H2 = 14
H3 = 15
H4 = 17
H5 = 20
H6 = 24

Traditionally, this would have required 10 page allocations. We couldn't make use of variable-length page sizes, because we only had access to a 1D array for addressing.

We can partition our address space using phext delimiters, enabling fine-grained access to a 9D virtual address space.

Here's an example series of memory requests that are mapped into a sequential subspace region.

1. Store S1 @ 1.1.1/1.1.1/1.1.1 -> S
2. Store H1 @ 5.1.1/1.1.1/1.1.1 -> S<LB><LB><LB><LB>H1
3. Store H2 @ 6.1.1/1.1.1/1.1.1 -> S<LB><LB><LB><LB>H1<LB>H2
4. Store H3 @ 7.1.1/1.1.1/1.1.1 -> S<LB><LB><LB><LB>H1<LB>H2<LB>H3
5. Store S2 @ 1.1.1/1.1.1/1.1.2 -> S<SB>S<LB><LB><LB><LB>H1<LB>H2<LB>H3
6. Store S3 @ 1.1.1/1.1.1/1.1.3 -> S<SB>S<SB>S<LB><LB><LB><LB>H1<LB>H2<LB>H3
7. Store H4 @ 9.1.1/1.1.1/1.1.1 -> S<SB>S<SB>S<LB><LB><LB><LB>H1<LB>H2<LB>H3<LB>H4
8. Store H5 @ 12.1.1/1.1.1/1.1.1 -> S<SB>S<SB>S<LB><LB><LB><LB>H1<LB>H2<LB>H3<LB>H4<LB>H5
9. Store H6 @ 16.1.1/1.1.1/1.1.1 -> S<SB>S<SB>S<LB><LB><LB><LB>H1<LB>H2<LB>H3<LB>H4<LB>H5<LB>H6
10. Store S4 @ 1.1.1/1.1.1/1.1.4 -> S<SB>S<SB>S<SB>S<LB><LB><LB><LB>H1<LB>H2<LB>H3<LB>H4<LB>H5<LB>H6

At the end of this series of operations, we've constructed a hierarchical map of memory that has well-defined semantics for how it expands as additional entries are added.

Phext allows us to reason about packing information topologically.

If your workload can't be cached, at least make sure it can be prefetched. On a DDR5 system, we can feed data to the CPU at 40 GB/sec (10 bytes per cycle). But there's a 300+ cycle penalty for requesting data. Caches generally prevent us from noticing that delay, but it creeps in.

TL;DR: If your workload is hard to cache, a 4 GHz CPU becomes a 10 MHz CPU. Phext helps ensure that every workload can be easily cached - because we have precise control over where information resides, relatively speaking.

The next step: eliminating the 10% cache miss rate on modern CPUs.

For an OS that is actively managing memory, we can afford to utilize 1 GHz of amortized compute in order to completely eliminate the remaining 10% miss rate. For a 4 GHz CPU with 8 cores, we're only able to feed the CPU at 28.8 GHz - leaving 3.2 GHz lost to poor system architecture decisions made over the last 50 years.

If we consume 30% of the typical cache miss budget, we will regain 2.2 GHz and boost performance by about 7.5%. This is a modest gain, but a free one! The more important gain, however, is not absolute performance but the elimination of lag.

Whenever we need to fetch data from RAM, access time skyrockets...
* L1: 0.3 cycles @ 95%: 0.285 cycles
* L2: 14 cycles @ 4.5%: 0.63 cycles
* L3: 50 cycles @ 0.3%: 0.15 cycles
* RAM: 300+ cycles @ 0.2% = 0.6 cycles
* SSD: 100,000 cycles

Average latency: 1.6 cycles
Maximum latency: 360 cyclesAugust 21, 2025
---------------
My war on latency starts with the cache hierarchy. For workloads that can't be cached, our hybrid system drops from 4000 MHz to 10 MHz. This is rare, but happens about 3 minutes every day.

When working on a latency-driven system, we don't measure absolute performance. Instead, we measure average and maximum response times.

On a modern system, we can vastly improve performance if we simply keep all data on the CPU at all times. Instead of reading from RAM, we'll just hallucinate the data we need by always storing data in compressed form in RAM.

Take a set of small integers (1-1000) as an example. We only need 10 bits to store each value. Typical values need to be 64-bit aligned. So an array of 1,000 integers consumes 64,000 bits (8 KB). Compressed, however, we only need 1.25 KB - giving us a compression ratio of 6:1.

Suddenly the 300 cycle delay for round-trips to RAM only costs 50 cycles - the same as L3...

Text compression can achieve 10:1 compression ratios, which means RAM should be faster than L3 (30 cycles instead of 50). So going forward, we'll disable L3 caching when we can and ensure that all memory access is heavily compressed.

L1: 0.3 cycles @ 95% = 0.285 cycles
L2: 14 cycles @ 4.5% = 0.63 cycles
Raw RAM: 300 cycles @ 0.5% = 1.5 cyclesCompressed RAM: 30 cycles @ 0.5% = 0.15 cycles

Average Latency: 1.06 cycles
Maximum Latency: 44.3 cycles

Our L3 cache just expanded from megabytes to gigabytes - simply by leveraging compression via execution resources sitting dormant most of the time anyway...

(It can't really be _that_ easy, right? Right!?)

Note that maximum latency above is a bit of an apples to oranges comparison, as we will have to pay a compression penalty...but note that L1 can service 2 operations per cycle, so we can get a lot of extra work done while waiting for data from RAM.

On a modern CPU, we can afford 3 reads from cache - jumping the effective clock rate to 12 GHz if our workload fits into L1 cache somehow. Let's evaluate what sort of operations are available to us with 920 cycles per compressed cache line (3 cached ops per clock x 307 cycles - see below).

Each cache line is 64 bytes. At 40 GB/sec from RAM, we're reading an average of 10 bytes per clock cycle (just 25% more than a 64-bit integer). So the transfer takes 7 clock cycles in addition to the 300 cycle penalty.

Let's make some tweaks to our logic that allow us to make assumptions about incoming data.
Registers: rax, rcx, rdx, rbx, rsp, rbp, rsi, rdi, r8, r9, r10, r11, r12, r13, r14, r15

On 64-bit x86 code, we have 16 registers available to us. This allows us to compress 4 bits out of every 64 bits, for free. All we have to do is control which registers we load values into.

0000: rax
0001: rcx
0010: rdx
0011: rbx
0100: rsp
0101: rbp
0110: rsi
0111: rdi
1000: r8
1001: r9
1010: r10
1011: r11
1100: r12
1101: r13
1110: r14
1111: r15

This loading trick nets us our first compression win: 6.25%. We can now store eight 64-bit integers using only 480 bits, leaving us with 32 bits free for other purposes, like compression.August 22, 2025
---------------
Yesterday we identified that we can save 4 bits per 64-bit integer load by controlling which x86-64 register we load values into (since there are 16 registers to choose from).

In our compression budget, we have 920 cycles available, and 32-bits of freedom (so far).

Each cache line is 512 bits (64 bytes).
Compressed cache lines, version 1, are 480 bits of data with 32-bits of metadata.

bbbb|bbb|bbbbbbbbbbbbbbbbbbbbbbbbb
0123|456|.........................

1. We'll use four bits to recover which register we're loading values into. This will be consistent for all of the integers found in a cache line. This means that we'll sort values into hash tables keyed from the first four bits of the value being stored.

2. The next three bits determine what sort of numbers we're dealing with. Based upon a range check, we set some flags to indicate how many integers were stored. Bits 4-6 encode the container size (8-bit, 16-bit, 24-bit, 32-bit, 40-bit, 48-bit, 56-bit, or 64-bit).

111 56-bit numbers:  8 integers [72,057,594,037,927,935]
110 48-bit numbers: 10 integers [281,474,976,710,655]
101 40-bit numbers: 12 integers [1,099,511,627,775]
100 32-bit numbers: 15 integers [4,294,967,295]
011 24-bit numbers: 20 integers [16,777,215]
010 16-bit numbers: 30 integers [65,535]
001  8-bit numbers: 60 integers [255]
000 64-bit numbers:  8 integers

We've used 7 of our 32 compression bits. Let's evaluate some tricks we can play on the data next.

For b456 = 0:
We'll make use of 8 groups of 3 bits to further describe the low-level bit string. Split our 480 bits into eight groups of 60 bits each. We'll XOR test patterns into these regions to determine if we can salvage some extra bits along the way.

For each 60-bit region, we'll check for numbers that are smaller than our max. The number of additional bytes generated depends upon the input, ranging from 0 to 52 extra bytes per 64 bytes of input.

111: 4 bits unused (56-bit) -> 32 additional bits -> 60+4 bytes
110: 12 bits unused (48-bit) -> 96 additional bits -> 52+12 bytes
101: 20 bits unused (40-bit) -> 160 additional bits -> 44+20 bytes
100: 28 bits unused (32-bit) -> 224 additional bits -> 36+28 bytes
011: 36 bits unused (24-bit) -> 288 additional bits -> 28+36 bytes
010: 44 bits unused (16-bit) -> 352 additional bits -> 20+44 bytes
001: 52 bits unused (8-bit) -> 416 additional bits -> 12+52 bytes
000: 0 bits unused (64-bit) -> no additional packing

We can now reclaim a variable number of bits, based upon how many zeros we find at the beginning of the value stored. If we're storing 8-bit integers in a 64-bit container, we can reclaim 416 bits - using just (32+64 = 96) bits for data and 416 bits for compression.

Each block can produce a variable number of additional bytes, which are written into the empty bits of the prior block.

We can read the effective length of our cache line by unpacking these 24 bits. Here are several examples.

000'000'000'000'000'000'000'000 -> 64 bytes
010'100'110'111'011'010'001'011 -> 96 bytes
001'001'001'001'001'001'001'001 -> 116 bytes

Without any compression overhead, we could store eight 8-bit integers in 8 bytes. Our compression overhead is only 4 bytes (12 bytes total).

For b456 = 001:
We will be packing 60 x 8-bit integers into our 480-bit container in this case. We will pack ten integers per group, resulting in 6 groups. We can use 4 bits to characterize each group, enabling us to save anywhere from 0 to 8 bits per group, for a maximum savings of 60 bytes.

For b456 = 010: 30
We will be packing 30 x 16-bit integers into our 480-bit container in this case. We will chunk groups of five integers at a time, resulting in 6 groups. Again, we'll characterize each group using a 4-bit indicator, allowing us to save at most 56 bytes. The 4-bit indicator in this case is just a count of the number of leading zeros we can remove from all entries in the group.

For b456 = 011: 20 24-bit
We will be packing 20 x 24-bit integers into our 480-bit container in this case. We'll chunk four integers at a time, resulting in 5 groups. We'll have 5 bits to characterize each group with, allowing us to save up to 60 bytes.

For b456 = 100: 15 32-bit
We will be packing 15 x 32-bit integers into our 480-bit container in this case. We will chunk 3 entries at a time, giving us 5 groups. We can use 5 bits per group, allowing us a maximum savings of 58 bytes (15x3.875).

For b456 = 101: 12 40-bit
We will be packing 12 x 40-bit integers into our 480-bit container in this case. We will chunk 2 entries at a time, giving us 6 groups. We can use 4 bits per group, allowing us to save up to 60 bytes. We'll count how many groups of three zeros we can omit.

For b456 = 110: 10 48-bit
We will be packing 10 x 48-bit integers into our 480-bit container in this case. We will chunk 2 entries at a time, giving us 5 groups. We can use 5 bits per group, allowing us to save up to 60 byets. We'll count how many groups of two zeros we can omit.

For b456 = 111: 8  56-bit
We will be packing 8 x 56-bit integers into our 480-bit container in this case. We will have three bits per entry to characterize input. We'll count how many groups of seven zeros we can omit.August 23, 2025
---------------
I'm working on a C implementation of cache line compression.
August 24, 2025
---------------
We went to the NE State Fair this weekend. Still iterating on cache line compression.
August 25, 2025
---------------
Just too damn busy...August 26, 2025
---------------
I'm focusing on encode/decode patterns for fast vectors.
August 27, 2025
---------------
I'll probably do another session on the Crazy Wisdom show soon.
Before then, I need to work out the details of proving L3 cache is ineffective at best.

I want to return to hallucinated data for a bit.
Given a hierarchical content hash, we can constrain how data is inflated.

Take this sequence, for example:

1.1.1/1.1.1/1.1.1: Hello World
1.1.1/1.1.1/1.1.2: This is a test of the emergency broadcast system.
1.4.5/8.3.2/4.4.7: Dust is the enemy of heat dissipation.

We can constraint this dataset by encoding a few facts, also hierarchically (the checksum is just the first four bytes of a sha1sum):

1.1.1/1.1.1/1.1.1: 11 bytes + 0a4d
1.1.1/1.1.1/1.1.2: 49 bytes + b061
1.4.5/8.3.2/4.4.7: 38 bytes + e40c

For large buffers, inflating from checksums and buffer lengths feels intractable...
But let's work with small buffers for a moment.

Given a 512-byte array, we'll chunk it into 32x16 bytes. For each 32-byte chunk, we'll place it within a phext address in subspace. This allows us to precisely locate a text expansion routine from a much larger set of known texts.

August 28, 2025
---------------
Low latency computing.
Minimum target: 1000 fps
On modern processors, we can plan for 16 threads easily (8C/16T on $1K boxes).
For each user-level thread, we'll have a hierarchy of worker threads.
These sub-threads coordinate different domains of application performance.
We keep work in separate queues to ensure we're avoiding blocking code paths.
These sub-threads are called Strands.

Strand 1: Mouse: Handles GUI interactions - mouse pointer tracking and events
Strand 2: Keyboard: Handles text input events
Strand 3: Display: Handles screen rendering and compositing
Strand 4: System: Handles inter-process communication
Strand 5: Disk Reader: Handles requests to read from disk(s)
Strand 6: Disk Writer: Handles requests to write to disk(s)
Strand 7: Hunter: Performs primary units of computation / application tasks
Strand 8: Gatherer: Collects results from other strands
Strand 9: Network Reader: Handles requests to read from network(s)
Strand 10: Network Writer: Handles requests to write to network(s)
Strand 11: Application-Specific strand #1
Strand 12: Application-Specific strand #2
Strand 13: Application-Specific strand #3
Strand 14: Application-Specific strand #4
Strand 15: Application-Specific strand #5
Strand 16: Application-Specific strand #6

Communication between programs happens along strand boundaries.
We'll prioritize extremely fast message passing and non-blocking I/O.
Messaging will be broken out into a series of latency-oriented queues.

Queue 1: 1 ns: 4 clock cycles [L1] -- 32 KB
Queue 2: 10 ns: 40 clock cycles [L2] -- 1 MB
Queue 3: 100 ns: 400 clock cycles [L3] -- 10 MB
Queue 4: 1 us: 4,000 clock cycles [RAM] -- 64 GB
Queue 5: 10 us: 40,000 clock cycles [RAM] -- 64 GB
Queue 6: 100 us: 400,000 clock cycles [Disk] -- 2 TB
Queue 7: 1 ms: 4M clock cycles [Disk] -- 2 TB
Queue 8: 10 ms: 40M clock cycles [Network] -- Unbounded
Queue 9: 100 ms: 400M clock cycles [Network] -- Unbounded
Queue 10: 1 s: 4B clock cycles [User] -- 2 TB

Generally-speaking, you'll need to queue items by both size and count.
We're essentially forcing the system to utilize QoS across all parts of the software stack.
Developers are forced to internalize the caching hierarchy instead of ignoring it.

Entry Size
----------
Queue 1: 8-32 bytes (4 cycles x 3 ops/cycle x 50% x 8 bytes = 48 bytes)
Queue 2: 32-256 bytes (40 cycles x 3 ops/cycle x 50% x 8 bytes = 480 bytes)
Queue 3: 256 bytes to 4 KB (400 cycles x 3 ops/cycle x 50% x 8 bytes = 4,800 bytes)
Queue 4: 4 KB to 16 KB (40 GB/sec x 1 us x 50% = 20 KB)
Queue 5: 16 KB to 128 KB (40 GB/sec x 10 us x 50% = 200 KB)
Queue 6: 128 KB to 256 KB (5 GB/sec x 0.1 ms x 50% = 256 KB)
Queue 7: 1 MB to 2 MB (5 GB/sec x 1 ms x 50% = 2.5 MB) - low latency
Queue 8: 2 MB 16 MB (5 GB/sec x 10 ms x 50% = 25 MB) - high latency
Queue 9: 16 MB to 256 MB (5 GB/sec x 100 ms x 50% = 256 MB) - high latency
Queue 10: 256 MB to 2 GB (5 GB/sec x 1 s x 50% = 2 GB) - high latency

Transfers larger than 2 GB will not be queued by the kernel until we have systems with more RAM.

Entry Count
-----------
Queue 1: 1K to 4K items (128 KB = 4x L1)
Queue 2: 4K to 16K items (4 MB = 4x L2)
Queue 3: 16K to 32K items (128 MB = 4x L3)
Queue 4: 32K to 64K items (1 GB = 1.5% RAM)
Queue 5: 64K to 128K items (16 GB = 25% RAM)
Queue 6: 128K to 256K items (64 GB = 1x RAM)
Queue 7: 256K to 512K items (1 TB = 50% Disk)
Queue 8: 512K to 1M items (16 TB = 8x Disk)
Queue 9: 1M to 2M items (512 TB = 256x Disk)
Queue 10: 2M to 4M items (8 PB = 4096x Disk)August 29, 2025
---------------
I'll name the process developed yesterday "Hierarchical Queueing".
The key insight is that arrays offer an implicit address.
August 30, 2025
---------------
Dean Memorial Commemorative Edition

Some tasks require decades of preparation.
Despite planning, the future never unfolds exactly the way you intended.

Hierarchical Queueing
---------------------
Arrays are great because we implicitly encode the offset index.
Whenever we read a byte from an array, we only need to maintain our 64-bit pointer.
Most systems today are limited to addressing 48-bits of memory on 64-bit architectures.
2^48 = 256 TB of RAM, but most systems peak at 256 GB of RAM (38-bit).
This leaves us with quite a bit of room for shenanigans in memory addresses.
We'll arbitrarily limit ourselves to fewer than 36 bits of RAM (64 GB) per process.
This gives us at least 12-28 bits to encode a position in our memory layout.

Optimizations
-------------
1. Processes that run in each queue size are limited in the amount of memory they can address.
2. This means we can arbitrarily limit our memory space for each type of process.
3. Q1 processes only need 16 bits (64 KB)
4. Q2 processes only need 20 bits ( 1 MB)
5. Q3 processes only need 24 bits (16 MB)
6. Q4 processes only need 28 bits (256 MB)
7. Q5 processes only need 32 bits ( 4 GB)
8. Q6 processes only need 36 bits (64 GB)
9. Q7 processes only need 40 bits ( 1 TB)
A. Q8 processes only need 44 bits (16 TB)
B. Q9 processes only need 46 bits (64 TB)
C. QA processes only need 48 bits (256 TB)

Operating systems tend to use impossible bits, so we'll have access to traversal patterns...

* Q1 = 32 bits - 4 billion TPs (16^8)
* Q2 = 28 bits - 256 million TPs (16^7)
* Q3 = 24 bits - 16 million TPs (16^6)
* Q4 = 20 bits - 1 million TPs (16^5)
* Q5 = 16 bits - 64K TPs (16^4)
* Q6 = 12 bits - 4K TPs (16^3)
* Q7 = 8 bits - 256 TPs (16^2)
* Q8 = 4 bits - 16 TPs (16^1)
* Q9 = 2 bits - 4 TPs (16^0.5)
* QA = 0 bits - just an array

Given a 64-bit memory address pointer, our custom allocator looks like this:

* Q1 = |RRRRRRRRRRRRRRRR|EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE|DDDDDDDDDDDDDDDD|
* Q2 = |RRRRRRRRRRRRRRRR|EEEEEEEEEEEEEEEEEEEEEEEEEEEE|DDDDDDDDDDDDDDDDDDDD|
* Q3 = |RRRRRRRRRRRRRRRR|EEEEEEEEEEEEEEEEEEEEEEEE|DDDDDDDDDDDDDDDDDDDDDDDD|
* Q4 = |RRRRRRRRRRRRRRRR|EEEEEEEEEEEEEEEEEE|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|
* Q5 = |RRRRRRRRRRRRRRRR|EEEEEEEEEEEEEE|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|
* Q6 = |RRRRRRRRRRRRRRRR|EEEEEEEEEEEE|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|
* Q7 = |RRRRRRRRRRRRRRRR|EEEEEEEE|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|
* Q8 = |RRRRRRRRRRRRRRRR|EEEE|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|
* Q9 = |RRRRRRRRRRRRRRRR|EE|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|
* QA = |RRRRRRRRRRRRRRRR|DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD|

Degrees of freedom while encoding 4-bit coordinate values by process type.
The extra degree of freedom is the orthogonal dimension we count via array semantics.* Q1: 9D
* Q2: 8D
* Q3: 7D
* Q4: 6D
* Q5: 5D
* Q6: 4D
* Q7: 3D
* Q8: 2D
* Q9: 1.5D
* QA: 1D

Let's work an example for a Q1 process. We're going to orient data at the location below.

Location = 8.7.3/4.4.9/6.4.1
Address = |RRRRRRRRRRRRRRRR|10000111001101000100100101100100|DDDDDDDDDDDDDDDD|

Blocks of memory in this allocation scheme are placed within a phext document.
From a memory address, it is feasible for us to compute a physical location.
The application programmer can then think about accessing data at a particular location in subspace.

Our programming language thus requires phext semantics for reserving blocks of memory.

Instead of C's new/delete, which make no demands upon the memory allocator...

W requires you to declare your intent. If you want to work with 9D memory addresses, you have to reduce your memory usage to at most 64 KB. The system is self-reinforcing in terms of efficiency. If you want to be lazy and use 48-bits of RAM, you'll just get normal arrays.

But if you want to be fast+nimble, you can work in 9D. :)